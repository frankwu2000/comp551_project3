{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "\n",
    "#load data set\n",
    "x_real = np.loadtxt(\"train_x.csv\", delimiter=\",\") # load from text \n",
    "y_real = np.loadtxt(\"train_y.csv\", delimiter=\",\") \n",
    "x_test_real = np.loadtxt(\"test_x.csv\", delimiter=\",\") # load from text \n",
    "\n",
    "#x = x.reshape(-1, 64, 64) # reshape \n",
    "#y = y.reshape(-1, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 40) (10000, 40)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 24, 25, 27, 28, 30, 32, 35, 36, 40, 42, 45, 48, 49, 54, 56, 63, 64, 72, 81]\n",
    "# lb = preprocessing.LabelBinarizer()\n",
    "# lb.fit(classes)\n",
    "\n",
    "\n",
    "# x_real_1 = np.where(x_real > 252, 1, 0)\n",
    "x_real_1 = x_real/255\n",
    "x_test_real = x_test_real/255\n",
    "#normalize value in x_train\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_real_1, y_real, test_size=0.2, random_state=42)\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(classes)\n",
    "y_train = lb.transform(y_train)\n",
    "y_test = lb.transform(y_test)\n",
    "print(y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (40000, 64, 64, 1)\n",
      "40000 train samples\n",
      "10000 test samples\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " 1200/40000 [..............................] - ETA: 67s - loss: 3.6871 - acc: 0.0242"
     ]
    }
   ],
   "source": [
    "#keras\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten,Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D , AveragePooling2D\n",
    "from keras import backend as K\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 64, 64\n",
    "\n",
    "batch_size = 100\n",
    "num_classes = 40\n",
    "epochs = 1\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 64, 64\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "#start building layers\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(128, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(512, (3, 3), padding=\"same\"))\n",
    "# model.add(Activation('relu'))\n",
    "\n",
    "# model.add(Conv2D(512, (3, 3), padding=\"same\"))\n",
    "# model.add(Activation('relu'))\n",
    "\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.SGD(lr=0.02),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model.fit(x_train, y_train,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,\n",
    "#           verbose=1,\n",
    "#           validation_data=(x_test, y_test))\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath=\"checkpoint.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, callbacks=callbacks_list, verbose=1,validation_data=(x_test, y_test))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# save_model(model,\"model3.json\",\"model3.h5\")\n",
    "# load_model = load_model(\"model3.json\",\"model3.h5\")\n",
    "# output_result(load_model,\"kaggle_output_model3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# save_model(model,\"model2.json\",\"model2.h5\")\n",
    "# load_model = load_model(\"model.json\",\"model.h5\")\n",
    "# output_result(load_model,\"kaggle_output_model1_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model_to_save, json_name,h5_name):\n",
    "#     serialize model to JSON\n",
    "    model_json = model_to_save.to_json()\n",
    "    with open(json_name, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model_to_save.save_weights(h5_name)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "    \n",
    "from keras.models import model_from_json  \n",
    "\n",
    "def load_model(json_name,h5_name):\n",
    "    # load json and create model\n",
    "    json_file = open(json_name, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(h5_name)\n",
    "    print(\"Loaded model from disk\")\n",
    "    return loaded_model\n",
    " \n",
    "def output_result(loaded_model,file_name):\n",
    "    global x_test_real\n",
    "    \n",
    "    x_test_real = x_test_real.reshape(x_test_real.shape[0], img_rows, img_cols,1)\n",
    "    output_kaggle = loaded_model.predict(x_test_real)\n",
    "    output_kaggle = lb.inverse_transform(output_kaggle)\n",
    "\n",
    "    with open(file_name,\"w\") as output:\n",
    "        output.write(\"Id\")\n",
    "        output.write(\",\")\n",
    "        output.write(\"Label\")\n",
    "        output.write(\"\\n\")\n",
    "        for row in range(len(output_kaggle)):\n",
    "            output.write(str(row+1))\n",
    "            output.write(\",\")\n",
    "            output.write(str(output_kaggle[row]))\n",
    "            output.write(\"\\n\")\n",
    "# # evaluate loaded model on test data\n",
    "# loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "# score = loaded_model.evaluate(x_test, y_test, verbose=1)\n",
    "# print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [(32, 5), (32, 3), (32, 1), (None, None)] [(32, 5), (32, 3), (32, 1), (None, None)] [256] True 0.1 False sgd True\n",
      "x_train shape: (37500, 64, 64, 1)\n",
      "37500 train samples\n",
      "12500 test samples\n",
      "Train on 37500 samples, validate on 12500 samples\n",
      "Epoch 1/30\n",
      "37500/37500 [==============================] - 58s - loss: 4.1265 - acc: 0.0491 - val_loss: 3.7108 - val_acc: 0.0383\n",
      "Epoch 2/30\n",
      "37500/37500 [==============================] - 53s - loss: 3.7541 - acc: 0.0870 - val_loss: 3.4872 - val_acc: 0.1149\n",
      "Epoch 3/30\n",
      "37500/37500 [==============================] - 53s - loss: 3.5905 - acc: 0.1050 - val_loss: 3.4483 - val_acc: 0.1206\n",
      "Epoch 4/30\n",
      "37500/37500 [==============================] - 53s - loss: 3.4706 - acc: 0.1203 - val_loss: 3.4118 - val_acc: 0.1196\n",
      "Epoch 5/30\n",
      "37500/37500 [==============================] - 54s - loss: 3.3667 - acc: 0.1338 - val_loss: 3.4035 - val_acc: 0.1242\n",
      "Epoch 6/30\n",
      "25700/37500 [===================>..........] - ETA: 15s - loss: 3.2663 - acc: 0.1503"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-11a39db65c45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    132\u001b[0m                             \u001b[1;32mfor\u001b[0m \u001b[0mdropout\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatchNorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_pool_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                                 \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatchNorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_pool_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m                                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-11a39db65c45>\u001b[0m in \u001b[0;36mbuild_model\u001b[1;34m(conv, conv2, dense, batchNorm, learning_rate, is_pool_max, optimizer, dropout)\u001b[0m\n\u001b[0;32m    108\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m               \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m               validation_data=(x_test, y_test))\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 867\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1598\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2273\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2274\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# #keras\n",
    "# from __future__ import print_function\n",
    "# import keras\n",
    "# from keras.datasets import mnist\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout, Flatten,Activation\n",
    "# from keras.layers import Conv2D, MaxPooling2D , AveragePooling2D\n",
    "# from keras import backend as K\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "# batch_size = 100\n",
    "# num_classes = 40\n",
    "# epochs = 30\n",
    "\n",
    "# # input image dimensions\n",
    "# img_rows, img_cols = 64, 64\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x_real_1, y_real, test_size=0.25, random_state=42)\n",
    "# lb = preprocessing.LabelBinarizer()\n",
    "# lb.fit(classes)\n",
    "# y_train = lb.transform(y_train)\n",
    "# y_test = lb.transform(y_test)\n",
    "\n",
    "\n",
    "# def build_model(conv,conv2, dense, batchNorm, learning_rate, is_pool_max, optimizer, dropout):\n",
    "#     global x_train, x_test, y_train, y_test\n",
    "#     if K.image_data_format() == 'channels_first':\n",
    "#         x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "#         x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "#         input_shape = (1, img_rows, img_cols)\n",
    "#     else:\n",
    "#         x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "#         x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "#         input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "#     x_train = x_train.astype('float32')\n",
    "#     x_test = x_test.astype('float32')\n",
    "#     x_train /= 255\n",
    "#     x_test /= 255\n",
    "#     print('x_train shape:', x_train.shape)\n",
    "#     print(x_train.shape[0], 'train samples')\n",
    "#     print(x_test.shape[0], 'test samples')\n",
    "\n",
    "#     # convert class vectors to binary class matrices\n",
    "#     # y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "#     # y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "#     model = Sequential()\n",
    "\n",
    "#     model.add(Conv2D(conv[0][0], kernel_size=(conv[0][1], conv[0][1]),padding='same',input_shape=input_shape))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Activation('relu'))\n",
    "    \n",
    "#     for conv_filter, kernel_size in conv[1:]:\n",
    "#         if conv_filter == None:\n",
    "#             if is_pool_max:\n",
    "#                 model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#             else:\n",
    "#                 model.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "#         else:\n",
    "#             model.add(Conv2D(conv_filter, (kernel_size, kernel_size),padding='same'))\n",
    "#             if batchNorm:\n",
    "#                 model.add(BatchNormalization())\n",
    "#             model.add(Activation('relu'))\n",
    "    \n",
    "\n",
    "#     for conv_filter, kernel_size in conv2:\n",
    "#         if conv_filter == None:\n",
    "#             if is_pool_max:\n",
    "#                 model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#             else:\n",
    "#                 model.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "#         else:\n",
    "#             model.add(Conv2D(conv_filter, (kernel_size, kernel_size),padding='same'))\n",
    "#             if batchNorm:\n",
    "#                 model.add(BatchNormalization())\n",
    "#             model.add(Activation('relu'))\n",
    "\n",
    "# #     model.add(Dropout(0.5))\n",
    "#     model.add(Flatten())\n",
    "\n",
    "#     for num_neuron in dense:\n",
    "        \n",
    "#         model.add(Dense(num_neuron))\n",
    "#         if batchNorm:\n",
    "#                 model.add(BatchNormalization())\n",
    "#         model.add(Activation('relu'))\n",
    "#         if dropout:\n",
    "#             model.add(Dropout(0.5))\n",
    "   \n",
    "\n",
    "#     model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "#     from keras import optimizers\n",
    "#     if optimizer == \"sdg\":\n",
    "#         optimizer = optimizers.SGD(lr=learning_rate)\n",
    "#     elif optimizer == \"adam\":\n",
    "#         optimizer = optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "#     else:\n",
    "#         optimizer = optimizers.Adadelta(lr=learning_rate, rho=0.95, epsilon=1e-08, decay=0.0)\n",
    "#     # model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "#     model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "#                   optimizer=optimizer,\n",
    "#                   metrics=['accuracy'])\n",
    "\n",
    "#     model.fit(x_train, y_train,\n",
    "#               batch_size=batch_size,\n",
    "#               epochs=epochs,\n",
    "#               verbose=1,\n",
    "#               validation_data=(x_test, y_test))\n",
    "\n",
    "#     score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "#     print('Test loss:', score[0])\n",
    "#     print('Test accuracy:', score[1])\n",
    "#     return (model, score)\n",
    "\n",
    "\n",
    "# convs = [\n",
    "#     [(32,5),(32,3),(32,1),(None, None)],\n",
    "#     [(32,5),(32,5),(32,5),(None, None)],\n",
    "#     [(64,5),(32,3),(None, None)]\n",
    "# ]\n",
    "# models = []\n",
    "# for conv in convs:\n",
    "#     for conv2 in convs:\n",
    "#         for dense in [[256],[512,256],[512,512,256]]:\n",
    "#             for batchNorm in (True,):\n",
    "#                 for learning_rate in [0.1]:\n",
    "#                     for is_pool_max in (True,False):\n",
    "#                         for optimizer in [\"sgd\"]:\n",
    "#                             for dropout in (True,):\n",
    "#                                 print(len(models) ,conv, conv2, dense, batchNorm, learning_rate, is_pool_max, optimizer, dropout)\n",
    "#                                 models.append(build_model(conv,conv2, dense, batchNorm, learning_rate, is_pool_max, optimizer, dropout))\n",
    "#                                 print(models[-1][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #keras\n",
    "# from __future__ import print_function\n",
    "# import keras\n",
    "# from keras.datasets import mnist\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout, Flatten,Activation\n",
    "# from keras.layers import Conv2D, MaxPooling2D\n",
    "# from keras import backend as K\n",
    "\n",
    "# batch_size = 128\n",
    "# num_classes = 10\n",
    "# epochs = 12\n",
    "\n",
    "# # input image dimensions\n",
    "# img_rows, img_cols = 28, 28\n",
    "\n",
    "# # the data, shuffled and split between train and test sets\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# if K.image_data_format() == 'channels_first':\n",
    "#     x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "#     x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "#     input_shape = (1, img_rows, img_cols)\n",
    "# else:\n",
    "#     x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "#     x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "#     input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# x_train = x_train.astype('float32')\n",
    "# x_test = x_test.astype('float32')\n",
    "# x_train /= 255\n",
    "# x_test /= 255\n",
    "# print('x_train shape:', x_train.shape)\n",
    "# print(x_train.shape[0], 'train samples')\n",
    "# print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# # convert class vectors to binary class matrices\n",
    "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "# y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(32, kernel_size=(3, 3),input_shape=input_shape))\n",
    "# model.add(Activation('relu'))\n",
    "\n",
    "# model.add(Conv2D(64, (3, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(Flatten())\n",
    "\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "#               optimizer=keras.optimizers.Adadelta(),\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.fit(x_train, y_train,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,\n",
    "#           verbose=1,\n",
    "#           validation_data=(x_test, y_test))\n",
    "\n",
    "# score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "# print('Test loss:', score[0])\n",
    "# print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
